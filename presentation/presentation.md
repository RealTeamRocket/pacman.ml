# Pr√§sentationsskript (5 Minuten) ‚Äì Pacman Reinforcement Learning

## Sprecheraufteilung
- **Paul Durz:** Folie 1‚Äì3 (Einf√ºhrung, Motivation, L√∂sungsansatz)
- **Manuel Holm:** Folie 4‚Äì5 (Architektur, Training & Reward Design)
- **Ron Seifried:** Folie 6‚Äì8 (Ergebnisse, Demo, Fazit)

---

# Paul Durz ‚Äì Folien 1‚Äì3

---

## Folie 1 ‚Äì Pacman Machine Learning (‚âà 30s)

**Stichworte**
- Begr√º√üung
- Projektname: Pacman Machine Learning
- Thema: Reinforcement Learning
- Zwei Ans√§tze: Q-Learning und Deep Q-Learning
- HFT Stuttgart, Modul ML & Data Mining
- Gruppenarbeit: Paul, Manuel, Ron

**Sprechtext**

Guten Tag und herzlich willkommen zu unserer Pr√§sentation. Wir sind Paul, Manuel und Ron, und wir stellen heute unser Projekt ‚ÄûPacman Machine Learning" vor.

In diesem Projekt haben wir Reinforcement Learning auf das klassische Pacman-Spiel angewandt. Dabei haben wir zwei verschiedene Ans√§tze implementiert: Q-Learning und Deep Q-Learning.

Diese Arbeit entstand im Rahmen des Moduls ‚ÄûMachine Learning und Data Mining" an der Hochschule f√ºr Technik Stuttgart.

**Background Info**
- Reinforcement Learning (RL): Ein Teilbereich des maschinellen Lernens, bei dem ein Agent durch Interaktion mit einer Umgebung lernt, optimale Entscheidungen zu treffen.
- Q-Learning: Tabellenbasierter RL-Algorithmus (1989, Watkins).
- Deep Q-Learning: Kombination von Q-Learning mit neuronalen Netzen (2013, DeepMind).

---

## Folie 2 ‚Äì Aufgabenstellung und Motivation (‚âà 45s)

**Stichworte**
- Themenbereich: Reinforcement Learning
- Lernende Agenten f√ºr Spiele entwickeln
- Belohnungssystem zur Umgebungsanalyse
- Autonome Entscheidungsfindung
- Warum Pacman? Klassisches Testbed
- Klare Regeln, Echtzeit-Entscheidungen
- Komplexe Gegner-Dynamik (Geister)
- Ziel: Agent lernt Punkte sammeln + Level gewinnen

**Sprechtext**

Unsere Aufgabe war es, lernende Agenten zu entwickeln, die das Pacman-Spiel selbstst√§ndig spielen k√∂nnen. Der Agent soll durch ein Belohnungssystem lernen, die Umgebung zu analysieren und autonome Entscheidungen zu treffen.

Warum haben wir Pacman gew√§hlt? Pacman ist ein klassisches Testbed f√ºr Game AI. Es hat klar definierte Regeln und Ziele, erfordert Echtzeit-Entscheidungen und bietet mit den Geistern eine komplexe Gegner-Dynamik.

Unser konkretes Ziel war: Ein Agent, der durch Erfahrung lernt, m√∂glichst viele Punkte zu sammeln und das Level zu gewinnen.

**Background Info**
- Testbed: Eine kontrollierte Umgebung zum Testen von Algorithmen.
- Game AI: K√ºnstliche Intelligenz f√ºr Spielumgebungen ‚Äì oft verwendet f√ºr RL-Forschung, da Spiele klare Belohnungsstrukturen haben.
- Echtzeit-Entscheidungen: Der Agent muss kontinuierlich handeln; keine Zeit f√ºr langes Nachdenken.
- R√ºckfrage ‚ÄûWarum nicht ein anderes Spiel?": Pacman bietet eine gute Balance aus Komplexit√§t und √úbersichtlichkeit, ist gut dokumentiert und hat eine √ºberschaubare Zustandsraum-Gr√∂√üe.

---

## Folie 3 ‚Äì L√∂sungsansatz (‚âà 45s)

**Stichworte**
- üéÆ Pacman in C: Fork von pacman.c, Mongoose f√ºr externe Steuerung
- üêç Python ML-Framework: HTTP-Kommunikation mit dem Spiel
- üìä Q-Learning: Tabellenbasiert, Junction-Entscheidungen
- üß† Deep Q-Learning: Neuronales Netz, Dueling-Architektur
- Tech-Stack: C99, Python, PyTorch, Mongoose, Poetry

**Sprechtext**

Unser L√∂sungsansatz besteht aus vier Komponenten:

Erstens: Das Pacman-Spiel selbst. Wir haben einen Fork des Open-Source-Projekts pacman.c verwendet und diesen mit Mongoose erweitert, um eine externe Steuerung zu erm√∂glichen.

Zweitens: Ein Python-basiertes ML-Framework. Unsere Agenten kommunizieren via HTTP mit dem C-Spiel.

Drittens: Ein Q-Learning-Agent. Dieser nutzt eine tabellenbasierte Strategie und trifft Entscheidungen an Kreuzungen ‚Äì sogenannten Junctions.

Viertens: Ein Deep Q-Learning-Agent. Dieser verwendet ein neuronales Netz mit Dueling-Architektur f√ºr bessere Generalisierung.

Unser Tech-Stack umfasst C99 f√ºr das Spiel, Python f√ºr die Agenten, PyTorch f√ºr Deep Learning, Mongoose f√ºr die HTTP-API und Poetry f√ºr das Dependency-Management.

**Background Info**
- pacman.c: Ein minimalistischer Pacman-Klon in C99, urspr√ºnglich von Flooh entwickelt.
- Mongoose: Eine eingebettete HTTP-Server-Bibliothek f√ºr C/C++.
- Dueling-Architektur: Eine DQN-Variante, die Value und Advantage getrennt sch√§tzt (Wang et al., 2016).
- Junction-Entscheidungen: Der Agent entscheidet nur an Kreuzungen, nicht bei jedem Frame ‚Äì reduziert die Komplexit√§t erheblich.
- R√ºckfrage ‚ÄûWarum HTTP statt direkter Integration?": Entkopplung erm√∂glicht unabh√§ngige Entwicklung und einfaches Debugging.

---

# Manuel Holm ‚Äì Folien 4‚Äì5

---

## Folie 4 ‚Äì Architektur und RL-Pipeline (‚âà 45s)

**Stichworte**
- Zustand: Position, Geister, Dots
- Agent: Q-Table oder DQN
- Aktion: Hoch, Runter, Links, Rechts
- Environment: Pacman.c
- Reward: +15 Dot, -200 Tod
- Trainingsschleife: Zustand ‚Üí Aktion ‚Üí Reward ‚Üí Update

**Sprechtext**

Hier sehen Sie unsere Reinforcement-Learning-Pipeline.

Der Agent erh√§lt einen Zustand aus der Umgebung. Dieser Zustand enth√§lt Informationen √ºber die aktuelle Position, die Geister und die verbleibenden Dots.

Basierend auf diesem Zustand w√§hlt der Agent eine Aktion ‚Äì entweder √ºber eine Q-Table beim klassischen Q-Learning oder √ºber ein neuronales Netz beim Deep Q-Learning. Die m√∂glichen Aktionen sind: Hoch, Runter, Links, Rechts.

Die Aktion wird an das Environment ‚Äì also das Pacman-Spiel ‚Äì gesendet. Das Environment gibt einen Reward zur√ºck. Zum Beispiel plus 15 f√ºr einen gesammelten Dot oder minus 200 bei Tod.

Dieser Reward wird genutzt, um die Policy des Agenten zu aktualisieren. Dann wiederholt sich der Zyklus.

**Background Info**
- Policy: Die Strategie des Agenten, die Zust√§nden Aktionen zuordnet.
- Q-Table: Eine Tabelle, die f√ºr jedes Zustands-Aktions-Paar einen Q-Wert speichert.
- DQN (Deep Q-Network): Ein neuronales Netz, das Q-Werte approximiert, wenn der Zustandsraum zu gro√ü f√ºr eine Tabelle ist.
- R√ºckfrage ‚ÄûWarum nur 4 Aktionen?": Pacman bewegt sich diskret in einem Gitter; diagonale Bewegungen gibt es nicht.

---

## Folie 5 ‚Äì Training und Reward Design (‚âà 45s)

**Stichworte**
- State-Repr√§sentation: 16 Zonen, Ausg√§nge, Geister-Distanz, Food-Richtung, Power-Modus
- Reward-Signale: +15 Dot, +40 Power-Pill, +100‚Äì1700 Geist, -10 bis -200 Tod, +2000 Level gewonnen
- Besonderheiten: 3-Leben-System, Survival-Penalty, Dot-Milestones, Junction-Entscheidungen
- Q-Update-Formel

**Sprechtext**

F√ºr das Training haben wir eine kompakte State-Repr√§sentation entwickelt. Das Spielfeld ist in 16 Zonen eingeteilt. Der Zustand enth√§lt: verf√ºgbare Ausg√§nge, Distanz und Richtung zum n√§chsten Geist, Richtung zum n√§chsten Food, und ob der Power-Modus aktiv ist.

Beim Reward Design haben wir differenzierte Signale definiert: Plus 15 pro Dot, plus 40 f√ºr eine Power-Pill, plus 100 bis 1700 f√ºr das Fressen eines Geistes ‚Äì je nachdem wie viele Geister hintereinander gefressen werden. Minus 10 bis minus 200 bei Tod, abh√§ngig von der Spielphase. Und plus 2000 f√ºr ein gewonnenes Level.

Zu den Besonderheiten: Jede Episode hat 3 Leben. Es gibt eine Survival-basierte Penalty, progressive Dot-Milestones als Zwischenbelohnungen, und der Agent entscheidet nur an Junctions.

Die Q-Update-Formel sehen Sie hier: Q von s,a wird aktualisiert mit Alpha mal der Differenz aus erhaltenem Reward plus diskontiertem maximalem Q-Wert des Folgezustands minus dem aktuellen Q-Wert.

**Background Info**
- Alpha (Lernrate): Bestimmt, wie stark neue Informationen gewichtet werden (typisch: 0.1‚Äì0.5).
- Gamma (Discount-Faktor): Gewichtet zuk√ºnftige Rewards (typisch: 0.9‚Äì0.99).
- Power-Modus: Nach Aufnahme einer Power-Pill kann Pacman f√ºr kurze Zeit Geister fressen.
- Survival-Penalty: L√§ngeres √úberleben ohne Punkte wird bestraft, um passives Verhalten zu vermeiden.
- R√ºckfrage ‚ÄûWarum Junction-basiert?": Reduziert die Anzahl der Entscheidungspunkte erheblich und macht das Learning effizienter.

---

# Ron Seifried ‚Äì Folien 6‚Äì8

---

## Folie 6 ‚Äì Ergebnisse und Beobachtungen (‚âà 45s)

**Stichworte**
- Q-Learning (5000 Episoden):
  - Durchschn. Score: 1238.3
  - Durchschn. Dots: 102.0
  - Best Score: 3500
  - Best Dots: 218
  - 200+ Dots: 7 Episoden
- Deep Q-Learning (2000 Episoden):
  - Durchschn. Score: 1981.7
  - Durchschn. Dots: 163.5
  - Best Score: 4490
  - Best Dots: 244
  - Gewonnene Runden: 48 (2.4%)
- Kernbeobachtung: Deep Q-Learning √ºbertrifft Q-Learning, generalisiert besser

**Sprechtext**

Kommen wir zu unseren Ergebnissen.

Bei Q-Learning nach 5000 Episoden erreichten wir einen durchschnittlichen Score von 1238 Punkten und durchschnittlich 102 gesammelte Dots. Der beste Score lag bei 3500, die beste Dot-Anzahl bei 218. In 7 Episoden wurden mehr als 200 Dots gesammelt.

Bei Deep Q-Learning ‚Äì hier nach 2000 Episoden ‚Äì sehen wir deutlich bessere Werte: Ein durchschnittlicher Score von fast 1982, durchschnittlich 163 Dots. Der beste Score erreichte 4490 Punkte, die beste Dot-Anzahl 244. Und wir hatten 48 gewonnene Runden, das entspricht 2,4 Prozent.

Die Kernbeobachtung: Deep Q-Learning √ºbertrifft Q-Learning deutlich. Es generalisiert besser √ºber den Zustandsraum, weil das neuronale Netz √§hnliche Zust√§nde √§hnlich behandeln kann.

**Background Info**
- Episoden: Ein komplettes Spiel von Start bis Game Over (oder Level gewonnen).
- Dots: Es gibt 244 Dots im Level; alle zu sammeln bedeutet Level gewonnen.
- Win-Rate 2.4%: Klingt niedrig, ist aber f√ºr RL-Agenten ohne Vorwissen ein respektables Ergebnis.
- R√ºckfrage ‚ÄûWarum weniger Episoden bei DQN?": DQN lernt effizienter pro Episode, ben√∂tigt aber mehr Rechenzeit pro Episode.

---

## Folie 7 ‚Äì Demo: Trainierte Agenten (‚âà 30s)

**Stichworte**
- Zwei GIFs nebeneinander
- Links: Q-Learning Agent (tabellenbasiert)
- Rechts: Deep Q-Learning Agent (Dueling DQN)
- Legende: Gelb = Pacman, Wei√ü = Dots, Rot = Geister
- Beobachtung: Unterschiedliche Spielstile

**Sprechtext**

Hier sehen Sie unsere beiden trainierten Agenten in Aktion.

Links der Q-Learning-Agent mit seiner tabellenbasierten Strategie. Rechts der Deep Q-Learning-Agent mit der Dueling-Architektur.

Gelb ist Pacman ‚Äì also unser Agent. Die wei√üen Punkte sind die Dots, die gesammelt werden m√ºssen. Und rot sind die Geister, denen der Agent ausweichen muss.

Beachten Sie die unterschiedlichen Spielstile: Der Q-Learning-Agent folgt oft festen Mustern, w√§hrend der Deep Q-Learning-Agent flexibler auf Situationen reagiert.

**Background Info**
- Die GIFs zeigen Timelapse-Aufnahmen aus dem Training.
- Tabellenbasiert: Deterministisch bei gleichem Zustand; kann zu repetitiven Mustern f√ºhren.
- DQN: Kann generalisieren; √§hnliche Zust√§nde f√ºhren zu √§hnlichem Verhalten.
- R√ºckfrage ‚ÄûWie lange dauert eine Episode?": Typisch 30‚Äì60 Sekunden Echtzeit, je nach Spielerfolg.

---

## Folie 8 ‚Äì Fazit (‚âà 30s)

**Stichworte**
- Erkenntnisse:
  - RL funktioniert f√ºr Echtzeit-Spiele
  - Reward-Shaping ist entscheidend
  - Deep Q-Learning skaliert besser als Q-Tables
  - Junction-Entscheidungen reduzieren Komplexit√§t
- Limitationen:
  - Lange Trainingszeiten
  - Hyperparameter-Tuning aufw√§ndig
  - Deterministisches Geisterverhalten
  - Multi-Ghost-Szenarien

**Sprechtext**

Zum Abschluss unser Fazit.

Unsere wichtigsten Erkenntnisse: Reinforcement Learning funktioniert f√ºr Echtzeit-Spiele wie Pacman. Das Reward-Shaping ‚Äì also das Design der Belohnungssignale ‚Äì ist entscheidend f√ºr den Lernerfolg. Deep Q-Learning skaliert besser als tabellenbasierte Ans√§tze. Und die Beschr√§nkung auf Junction-Entscheidungen reduziert die Komplexit√§t erheblich.

Zu den Limitationen: Das Training ben√∂tigt viel Zeit. Das Hyperparameter-Tuning ist aufw√§ndig. In unserem Setup verhalten sich die Geister deterministisch, was das Problem vereinfacht. Und Multi-Ghost-Szenarien mit mehr als einem Geist sind deutlich schwieriger.

Vielen Dank f√ºr Ihre Aufmerksamkeit. Wir freuen uns auf Ihre Fragen.

**Background Info**
- Trainingszeit: Q-Learning ca. 2‚Äì3 Stunden f√ºr 5000 Episoden; DQN ca. 4‚Äì6 Stunden f√ºr 2000 Episoden (auf CPU).
- Deterministisches Geisterverhalten: In unserem Setup folgen Geister festen Regeln; im Original-Pacman gibt es auch zuf√§llige Elemente.
- R√ºckfrage ‚ÄûWas w√§ren n√§chste Schritte?": Mehr Geister, nicht-deterministisches Verhalten, Transfer auf andere Level, Multi-Agent-Szenarien.
- R√ºckfrage ‚ÄûK√∂nnte der Agent das Spiel perfekt spielen?": Theoretisch ja, praktisch limitiert durch Zustandsraum-Gr√∂√üe und Trainingszeit.

---

# Timing-√úbersicht

| Folie | Titel                          | Sprecher | Zeit   |
|-------|--------------------------------|----------|--------|
| 1     | Pacman Machine Learning        | Paul     | 30s    |
| 2     | Aufgabenstellung und Motivation| Paul     | 45s    |
| 3     | L√∂sungsansatz                  | Paul     | 45s    |
| 4     | Architektur und RL-Pipeline   | Manuel   | 45s    |
| 5     | Training und Reward Design     | Manuel   | 45s    |
| 6     | Ergebnisse und Beobachtungen   | Ron      | 45s    |
| 7     | Demo: Trainierte Agenten       | Ron      | 30s    |
| 8     | Fazit                          | Ron      | 30s    |
| **Œ£** |                                |          | **5:15** |

*Hinweis: Pufferzeit von ca. 15 Sekunden f√ºr √úberg√§nge und eventuelle Nachfragen einplanen.*